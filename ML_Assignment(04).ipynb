{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1XqkaBcyo8B",
        "outputId": "32ccf538-b923-44f4-d7f8-3ef4af67a9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (17379, 17)\n",
            "\n",
            "Training RandomForest (Bagging)\n",
            "\n",
            "Training Subagging (BaggingRegressor)\n",
            "\n",
            "Training GradientBoosting (Boosting)\n",
            "\n",
            "Training XGBoost (Boosting)\n",
            "\n",
            "Cross Validation Results:\n",
            "                          Model  RMSE_mean  RMSE_std  MAE_mean   MAE_std\n",
            "0        RandomForest (Bagging)   2.674723  0.557160  0.952002  0.038472\n",
            "1  Subagging (BaggingRegressor)   2.750838  0.579156  0.938962  0.044020\n",
            "2   GradientBoosting (Boosting)   4.756997  0.275157  2.835052  0.097847\n",
            "3            XGBoost (Boosting)   5.961110  0.357773  3.347698  0.077641\n",
            "\n",
            "Best Model: RandomForest (Bagging)\n",
            "final_predictions.csv created!\n",
            "\n",
            "Top 8 Important Features:\n",
            "registered    0.947707\n",
            "casual        0.052111\n",
            "hum           0.000030\n",
            "windspeed     0.000026\n",
            "mnth          0.000023\n",
            "hr            0.000021\n",
            "temp          0.000020\n",
            "atemp         0.000019\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# Bike Demand Forecasting\n",
        "# Bagging vs Subagging vs Boosting (Regression)\n",
        "# Dataset Path: /content/hour.csv\n",
        "# ==========================================================\n",
        "\n",
        "# Install XGBoost if needed\n",
        "!pip install -q xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Load Dataset\n",
        "# ----------------------------------------------------------\n",
        "DATA_PATH = \"/content/hour.csv\"\n",
        "TARGET = \"cnt\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df = df.drop(columns=[\"instant\", \"dteday\"], errors=\"ignore\")\n",
        "\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. K-Fold Cross Validation\n",
        "# ----------------------------------------------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Ensemble Models\n",
        "# ----------------------------------------------------------\n",
        "models = {\n",
        "\n",
        "    \"RandomForest (Bagging)\": RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    \"Subagging (BaggingRegressor)\": BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=200,\n",
        "        max_samples=0.7,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "\n",
        "    \"GradientBoosting (Boosting)\": GradientBoostingRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    \"XGBoost (Boosting)\": XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Cross Validation Training\n",
        "# ----------------------------------------------------------\n",
        "for name, model in models.items():\n",
        "\n",
        "    rmse_scores = []\n",
        "    mae_scores = []\n",
        "\n",
        "    print(f\"\\nTraining {name}\")\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "        mae = mean_absolute_error(y_val, preds)\n",
        "\n",
        "        rmse_scores.append(rmse)\n",
        "        mae_scores.append(mae)\n",
        "\n",
        "    results.append([\n",
        "        name,\n",
        "        np.mean(rmse_scores),\n",
        "        np.std(rmse_scores),\n",
        "        np.mean(mae_scores),\n",
        "        np.std(mae_scores)\n",
        "    ])\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Save CV Results\n",
        "# ----------------------------------------------------------\n",
        "results_df = pd.DataFrame(results, columns=[\n",
        "    \"Model\",\n",
        "    \"RMSE_mean\",\n",
        "    \"RMSE_std\",\n",
        "    \"MAE_mean\",\n",
        "    \"MAE_std\"\n",
        "])\n",
        "\n",
        "results_df.to_csv(\"/content/cv_regression_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nCross Validation Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6. Train Best Model\n",
        "# ----------------------------------------------------------\n",
        "best_model_name = results_df.sort_values(\"RMSE_mean\").iloc[0][\"Model\"]\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(\"\\nBest Model:\", best_model_name)\n",
        "\n",
        "best_model.fit(X, y)\n",
        "\n",
        "predictions = best_model.predict(X)\n",
        "\n",
        "final_df = pd.DataFrame({\n",
        "    \"ActualCnt\": y,\n",
        "    \"PredictedCnt\": predictions\n",
        "})\n",
        "\n",
        "final_df.to_csv(\"/content/final_predictions.csv\", index=False)\n",
        "\n",
        "print(\"final_predictions.csv created!\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 7. Feature Importance (Top 8)\n",
        "# ----------------------------------------------------------\n",
        "if hasattr(best_model, \"feature_importances_\"):\n",
        "\n",
        "    importance = pd.Series(\n",
        "        best_model.feature_importances_,\n",
        "        index=X.columns\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\nTop 8 Important Features:\")\n",
        "    print(importance.head(8))\n"
      ]
    }
  ]
}